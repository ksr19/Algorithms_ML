{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем класс узла\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле\n",
    "        self.t = t  # значение порога\n",
    "        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле\n",
    "        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле\n",
    "\n",
    "# И класс терминального узла (листа)\n",
    "\n",
    "class Leaf:\n",
    "    \n",
    "    def __init__(self, data, labels, weights):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.prediction = self.predict()\n",
    "        \n",
    "    def predict(self):\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = {}  # сформируем словарь \"класс: количество объектов\"\n",
    "        for label in self.labels:\n",
    "            if label not in classes:\n",
    "                classes[label] = 0\n",
    "            classes[label] += 1\n",
    "            \n",
    "        # найдем класс, количество объектов которого будет максимальным в этом листе и вернем его    \n",
    "        prediction = max(classes, key=classes.get)\n",
    "        return prediction\n",
    "    \n",
    "\n",
    "    \n",
    "def weighted_gini(labels, weights):\n",
    "    #  подсчет количества объектов разных классов\n",
    "    classes = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in classes:\n",
    "            classes[labels[i]] = 0\n",
    "        classes[labels[i]] += weights[i]\n",
    "    \n",
    "    #  расчет критерия\n",
    "    impurity = 1\n",
    "    for label in classes:\n",
    "        p = classes[label] / sum(classes.values())\n",
    "        impurity -= p ** 2\n",
    "        \n",
    "    return impurity\n",
    "\n",
    "\n",
    "# Расчет прироста\n",
    "\n",
    "def gain(left_labels, right_labels, left_weights, right_weights, root_gini):\n",
    "\n",
    "    # доля выборки, ушедшая в левое поддерево\n",
    "    p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "    \n",
    "    return root_gini - p * weighted_gini(left_labels, left_weights) - (1 - p) * weighted_gini(right_labels, right_weights)\n",
    "\n",
    "# Разбиение датасета в узле\n",
    "\n",
    "def split(data, labels, weights, column_index, t):\n",
    "    \n",
    "    left = np.where(data[:, column_index] <= t)\n",
    "    right = np.where(data[:, column_index] > t)\n",
    "        \n",
    "    true_data = data[left]\n",
    "    false_data = data[right]\n",
    "    \n",
    "    true_labels = labels[left]\n",
    "    false_labels = labels[right]\n",
    "    \n",
    "    true_weights = weights[left]\n",
    "    false_weights = weights[right]\n",
    "        \n",
    "    return true_data, false_data, true_labels, false_labels, true_weights, false_weights\n",
    "\n",
    "\n",
    "# Нахождение наилучшего разбиения\n",
    "\n",
    "def find_best_split(data, labels, weights, min_samples_leaf):\n",
    "    \n",
    "    root_gini = weighted_gini(labels, weights)\n",
    "\n",
    "    best_gain = 0\n",
    "    best_t = None\n",
    "    best_index = None\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    for index in range(n_features):\n",
    "        # будем проверять только уникальные значения признака, исключая повторения\n",
    "        t_values = np.unique(data[:, index])\n",
    "        \n",
    "        for t in t_values:\n",
    "            true_data, false_data, true_labels, false_labels, true_weights, false_weights = split(data, labels, weights, index, t)\n",
    "            #  пропускаем разбиения, в которых в узле остается менее заданного количества объектов\n",
    "            if len(true_data) < min_samples_leaf or len(false_data) < min_samples_leaf:\n",
    "                continue\n",
    "            \n",
    "            current_gain = gain(true_labels, false_labels, true_weights, false_weights, root_gini)\n",
    "            \n",
    "            #  выбираем порог, на котором получается максимальный прирост качества\n",
    "            if current_gain > best_gain:\n",
    "                best_gain, best_t, best_index = current_gain, t, index\n",
    "\n",
    "    return best_gain, best_t, best_index\n",
    "\n",
    "# Построение дерева с помощью рекурсивной функции\n",
    "\n",
    "def build_tree(data, labels, weights, max_depth=1, min_samples_leaf=1):\n",
    "\n",
    "    gain, t, index = find_best_split(data, labels, weights, min_samples_leaf)\n",
    "\n",
    "    #  Базовый случай - прекращаем рекурсию, когда нет прироста в качестве, или же мы достигли максимальной глубины\n",
    "    if (gain == 0) or (max_depth == 0):\n",
    "        return Leaf(data, labels, weights)\n",
    "\n",
    "    true_data, false_data, true_labels, false_labels, true_weights, false_weights = split(data, labels, weights, index, t)\n",
    "\n",
    "    # Рекурсивно строим два поддерева, уменьшаем параметр max_depth на 1\n",
    "    true_branch = build_tree(true_data, true_labels, true_weights, max_depth-1, min_samples_leaf)\n",
    "    false_branch = build_tree(false_data, false_labels, false_weights, max_depth-1, min_samples_leaf)\n",
    "    \n",
    "    \n",
    "    # Возвращаем класс узла со всеми поддеревьями\n",
    "    return Node(index, t, true_branch, false_branch)\n",
    "\n",
    "\n",
    "def classify_object(obj, node):\n",
    "\n",
    "    #  Останавливаем рекурсию, если достигли листа\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.prediction\n",
    "\n",
    "    if obj[node.index] <= node.t:\n",
    "        return classify_object(obj, node.true_branch)\n",
    "    else:\n",
    "        return classify_object(obj, node.false_branch)\n",
    "    \n",
    "def predict(data, tree):\n",
    "    \n",
    "    classes = []\n",
    "    for obj in data:\n",
    "        prediction = classify_object(obj, tree)\n",
    "        classes.append(prediction)\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_of_say(x):\n",
    "    return 1/2 * np.log((1 - x) / x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_predict(X, trees, alphas):\n",
    "    \n",
    "    predictions = []\n",
    "    for tree in trees:\n",
    "        predictions.append(predict(X, tree))\n",
    "\n",
    "    # сформируем список с предсказаниями для каждого объекта\n",
    "    predictions_per_object = list(zip(*predictions))\n",
    "    #     print(predictions_per_object)\n",
    "\n",
    "    # выберем в качестве итогового предсказания вероятности для каждого объекта отношение количества объектов 1 класса ко всем объектам\n",
    "    final_predictions = []\n",
    "    for obj in predictions_per_object:\n",
    "        classes = {}\n",
    "        for i in range(len(obj)):\n",
    "            if obj[i] not in classes:\n",
    "                    classes[obj[i]] = 0\n",
    "            classes[obj[i]] += alphas[i]\n",
    "        final_predictions.append(max(classes, key=classes.get))\n",
    "    \n",
    "\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_fit(n_trees, X_train, y_train, positive_class=1, min_samples_leaf=1):\n",
    "    \n",
    "    # Деревья будем записывать в список\n",
    "    trees = []\n",
    "    alphas = []\n",
    "    \n",
    "    weights = np.array([1/X_train.shape[0]] * X_train.shape[0])\n",
    "    \n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        \n",
    "        tree = build_tree(X_train, y_train, weights, max_depth=1, min_samples_leaf=min_samples_leaf)       \n",
    "        predictions = predict(X_train, tree)\n",
    "        wrong_predictions = predictions != y_train\n",
    "        error = np.sum(weights[wrong_predictions])\n",
    "        alpha = amount_of_say(error)\n",
    "        weights[wrong_predictions] *= np.exp(alpha)\n",
    "        weights[~wrong_predictions] *= np.exp(-alpha)\n",
    "        weights /= sum(weights)\n",
    "        \n",
    "        trees.append(tree)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        \n",
    "        \n",
    "    return trees, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, n_features=4, n_informative=4, n_classes=2, n_redundant=0, \n",
    "                           n_clusters_per_class=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -  0.8945\n",
      "Test -  0.8835\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Train - ', accuracy_score(y_train, y_pred_train))\n",
    "print('Test - ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees, alphas = ab_fit(n_trees=50, X_train=X_train, y_train=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -  0.8485\n",
      "Test -  0.846\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_ab = ab_predict(X_train, trees, alphas)\n",
    "y_pred_ab = ab_predict(X_test, trees, alphas)\n",
    "print('Train - ', accuracy_score(y_train, y_pred_train_ab))\n",
    "print('Test - ', accuracy_score(y_test, y_pred_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_predict_proba(X, trees, alphas, positive_class=1):\n",
    "    predictions = []\n",
    "    for tree in trees:\n",
    "        predictions.append(predict(X, tree))\n",
    "\n",
    "    # сформируем список с предсказаниями для каждого объекта\n",
    "    predictions_per_object = list(zip(*predictions))\n",
    "    #     print(predictions_per_object)\n",
    "\n",
    "    # выберем в качестве итогового предсказания вероятности для каждого объекта отношение количества объектов 1 класса ко всем объектам\n",
    "    final_predictions = []\n",
    "    for obj in predictions_per_object:\n",
    "        classes = {}\n",
    "        for i in range(len(obj)):\n",
    "            if obj[i] not in classes:\n",
    "                classes[obj[i]] = 0\n",
    "            classes[obj[i]] += alphas[i]\n",
    "        final_predictions.append(classes.get(positive_class, 0)/sum(classes.values()))\n",
    "\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -  0.9645425407195114\n",
      "Test -  0.9564990933871296\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "print('Train - ', roc_auc_score(y_train, y_pred_train_proba[:, 1]))\n",
    "print('Test - ', roc_auc_score(y_test, y_pred_proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -  0.8817303613395886\n",
      "Test -  0.8746857875924126\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_ab_proba = ab_predict_proba(X_train, trees, alphas)\n",
    "y_pred_ab_proba = ab_predict_proba(X_test, trees, alphas)\n",
    "print('Train - ', roc_auc_score(y_train, y_pred_train_ab_proba))\n",
    "print('Test - ', roc_auc_score(y_test, y_pred_ab_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1, ..., -1,  1,  1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = y_train\n",
    "u_train[u_train == 0] = -1\n",
    "u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        X_column = X[:, self.feature_idx]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[X_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[X_column > self.threshold] = -1\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class Adaboost():\n",
    "\n",
    "    def __init__(self, n_clf=50):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "\n",
    "            min_error = float('inf')\n",
    "            # greedy search to find best threshold and feature\n",
    "            for feature_i in range(n_features):\n",
    "                X_column = X[:, feature_i]\n",
    "                thresholds = np.unique(X_column)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    # predict with polarity 1\n",
    "                    p = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[X_column < threshold] = -1\n",
    "\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    misclassified = w[y != predictions]\n",
    "                    error = sum(misclassified)\n",
    "\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # store the best configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_idx = feature_i\n",
    "                        min_error = error\n",
    "\n",
    "            # calculate alpha\n",
    "            EPS = 1e-10\n",
    "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
    "\n",
    "            # calculate predictions and update weights\n",
    "            predictions = clf.predict(X)\n",
    "\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
    "        y_pred = np.sum(clf_preds, axis=0)\n",
    "        y_pred = np.sign(y_pred)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "ab = Adaboost()\n",
    "ab.fit(X_train, u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8941726157398047"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_t = ab.predict(X_train)\n",
    "y_pred_t[y_pred_t == -1] = 0\n",
    "y_pred = ab.predict(X_test)\n",
    "y_pred[y_pred == -1] = 0\n",
    "roc_auc_score(y_train, y_pred_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8841817068338197"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train -  0.9579093322439503\n",
      "Test -  0.9521941832678891\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=42, algorithm='SAMME')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train_proba = model.predict_proba(X_train)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "print('Train - ', roc_auc_score(y_train, y_pred_train_proba[:, 1]))\n",
    "print('Test - ', roc_auc_score(y_test, y_pred_proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "weights = np.array([1/X_train.shape[0]] * X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DecisionTreeClassifier' has no attribute '_check_sample_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-59e9b414e1c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DecisionTreeClassifier' has no attribute '_check_sample_weight'"
     ]
    }
   ],
   "source": [
    "DecisionTreeClassifier._check_sample_weight(weights, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444445"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_gini([1, 2, 1], [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуем класс узла\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index  # индекс признака, по которому ведется сравнение с порогом в этом узле\n",
    "        self.t = t  # значение порога\n",
    "        self.true_branch = true_branch  # поддерево, удовлетворяющее условию в узле\n",
    "        self.false_branch = false_branch  # поддерево, не удовлетворяющее условию в узле\n",
    "\n",
    "# И класс терминального узла (листа)\n",
    "\n",
    "class Leaf:\n",
    "    \n",
    "    def __init__(self, data, labels, weights, positive_class=1):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.positive_class = positive_class\n",
    "        self.prediction = self.predict()\n",
    "        self.pred_proba = self.predict_proba()\n",
    "        \n",
    "    def predict(self):\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = {}  # сформируем словарь \"класс: количество объектов\"\n",
    "        for i in range(len(self.labels)):\n",
    "            if self.labels[i] not in classes:\n",
    "                classes[self.labels[i]] = 0\n",
    "            classes[self.labels[i]] += self.weights[i]\n",
    "            \n",
    "        # найдем класс, количество объектов которого будет максимальным в этом листе и вернем его    \n",
    "        prediction = max(classes, key=classes.get)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self):\n",
    "        # подсчет количества объектов разных классов\n",
    "        classes = {}  # сформируем словарь \"класс: количество объектов\"\n",
    "        for i in range(len(self.labels)):\n",
    "            if self.labels[i] not in classes:\n",
    "                classes[self.labels[i]] = 0\n",
    "            classes[self.labels[i]] += self.weights[i]\n",
    "            \n",
    "        return classes.get(self.positive_class, 0) / sum(classes.values())\n",
    "            \n",
    "        # найдем класс, количество объектов которого будет максимальным в этом листе и вернем его    \n",
    "#         prediction = max(classes, key=classes.get)\n",
    "#         return prediction\n",
    "    \n",
    "\n",
    "    \n",
    "def weighted_gini(labels, weights):\n",
    "    #  подсчет количества объектов разных классов\n",
    "    classes = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in classes:\n",
    "            classes[labels[i]] = 0\n",
    "        classes[labels[i]] += weights[i]\n",
    "    \n",
    "    #  расчет критерия\n",
    "    impurity = 1\n",
    "    for label in classes:\n",
    "        p = classes[label] / sum(classes.values())\n",
    "        impurity -= p ** 2\n",
    "        \n",
    "    return impurity\n",
    "\n",
    "\n",
    "# Расчет прироста\n",
    "\n",
    "def gain(left_labels, right_labels, left_weights, right_weights, root_gini):\n",
    "\n",
    "    # доля выборки, ушедшая в левое поддерево\n",
    "    p = float(left_labels.shape[0]) / (left_labels.shape[0] + right_labels.shape[0])\n",
    "    \n",
    "    return root_gini - p * weighted_gini(left_labels, left_weights) - (1 - p) * weighted_gini(right_labels, right_weights)\n",
    "\n",
    "# Разбиение датасета в узле\n",
    "\n",
    "def split(data, labels, weights, column_index, t):\n",
    "    \n",
    "    left = np.where(data[:, column_index] <= t)\n",
    "    right = np.where(data[:, column_index] > t)\n",
    "        \n",
    "    true_data = data[left]\n",
    "    false_data = data[right]\n",
    "    \n",
    "    true_labels = labels[left]\n",
    "    false_labels = labels[right]\n",
    "    \n",
    "    true_weights = weights[left]\n",
    "    false_weights = weights[right]\n",
    "        \n",
    "    return true_data, false_data, true_labels, false_labels, true_weights, false_weights\n",
    "\n",
    "\n",
    "# Нахождение наилучшего разбиения\n",
    "\n",
    "def find_best_split(data, labels, weights):\n",
    "    \n",
    "    root_gini = weighted_gini(labels, weights)\n",
    "\n",
    "    best_gain = 0\n",
    "    best_t = None\n",
    "    best_index = None\n",
    "    \n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    for index in range(n_features):\n",
    "        # будем проверять только уникальные значения признака, исключая повторения\n",
    "        t_values = np.unique(data[:, index])\n",
    "        \n",
    "        for t in t_values:\n",
    "            true_data, false_data, true_labels, false_labels, true_weights, false_weights = split(data, labels, weights, index, t)\n",
    "            \n",
    "            current_gain = gain(true_labels, false_labels, true_weights, false_weights, root_gini)\n",
    "            \n",
    "            #  выбираем порог, на котором получается максимальный прирост качества\n",
    "            if current_gain > best_gain:\n",
    "                best_gain, best_t, best_index = current_gain, t, index\n",
    "\n",
    "    return best_gain, best_t, best_index\n",
    "\n",
    "# Построение дерева с помощью рекурсивной функции\n",
    "\n",
    "def build_tree(data, labels, weights, max_depth=1):\n",
    "\n",
    "    gain, t, index = find_best_split(data, labels, weights)\n",
    "\n",
    "    #  Базовый случай - прекращаем рекурсию, когда нет прироста в качестве, или же мы достигли максимальной глубины\n",
    "    if (gain == 0) or (max_depth == 0):\n",
    "        return Leaf(data, labels, weights)\n",
    "\n",
    "    true_data, false_data, true_labels, false_labels, true_weights, false_weights = split(data, labels, weights, index, t)\n",
    "\n",
    "    # Рекурсивно строим два поддерева, уменьшаем параметр max_depth на 1\n",
    "    true_branch = build_tree(true_data, true_labels, true_weights, max_depth-1)\n",
    "    false_branch = build_tree(false_data, false_labels, false_weights, max_depth-1)\n",
    "    \n",
    "    \n",
    "    # Возвращаем класс узла со всеми поддеревьями\n",
    "    return Node(index, t, true_branch, false_branch)\n",
    "\n",
    "\n",
    "def classify_object(obj, node):\n",
    "\n",
    "    #  Останавливаем рекурсию, если достигли листа\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.prediction\n",
    "\n",
    "    if obj[node.index] <= node.t:\n",
    "        return classify_object(obj, node.true_branch)\n",
    "    else:\n",
    "        return classify_object(obj, node.false_branch)\n",
    "    \n",
    "    \n",
    "def predict(data, tree):\n",
    "    \n",
    "    classes = []\n",
    "    for obj in data:\n",
    "        prediction = classify_object(obj, tree)\n",
    "        classes.append(prediction)\n",
    "    return classes\n",
    "\n",
    "def get_proba_object(obj, node):\n",
    "\n",
    "    #  Останавливаем рекурсию, если достигли листа\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.pred_proba\n",
    "\n",
    "    if obj[node.index] <= node.t:\n",
    "        return get_proba_object(obj, node.true_branch)\n",
    "    else:\n",
    "        return get_proba_object(obj, node.false_branch)\n",
    "\n",
    "\n",
    "def predict_proba(data, tree):\n",
    "    \n",
    "    probas = []\n",
    "    for obj in data:\n",
    "        prob = get_proba_object(obj, tree)\n",
    "        probas.append(prob)\n",
    "    return probas\n",
    "\n",
    "def amount_of_say(x):\n",
    "    return 1/2 * np.log((1 - x) / x)\n",
    "\n",
    "def ab_predict(X, trees, alphas):\n",
    "    \n",
    "    predictions = []\n",
    "    for tree in trees:\n",
    "        predictions.append(predict(X, tree))\n",
    "\n",
    "    # сформируем список с предсказаниями для каждого объекта\n",
    "    predictions_per_object = list(zip(*predictions))\n",
    "    #     print(predictions_per_object)\n",
    "\n",
    "    # выберем в качестве итогового предсказания вероятности для каждого объекта отношение количества объектов 1 класса ко всем объектам\n",
    "    final_predictions = []\n",
    "    for obj in predictions_per_object:\n",
    "        classes = {}\n",
    "        for i in range(len(obj)):\n",
    "            if obj[i] not in classes:\n",
    "                    classes[obj[i]] = 0\n",
    "            classes[obj[i]] += alphas[i]\n",
    "        final_predictions.append(max(classes, key=classes.get))\n",
    "    \n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "def ab_fit(n_trees, X_train, y_train, positive_class=1):\n",
    "    \n",
    "    # Деревья будем записывать в список\n",
    "    trees = []\n",
    "    alphas = []\n",
    "    \n",
    "    weights = np.array([1/X_train.shape[0]] * X_train.shape[0])\n",
    "    \n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        \n",
    "        tree = build_tree(X_train, y_train, weights, max_depth=1)\n",
    "#         tree=DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "#         tree.fit(X_train, y_train, sample_weight=weights)\n",
    "#         predictions = tree.predict(X_train)\n",
    "        predictions = np.array(predict(X_train, tree))\n",
    "        wrong_predictions = predictions != y_train\n",
    "        error = sum(weights[wrong_predictions])\n",
    "        alpha = amount_of_say(error)\n",
    "        weights[wrong_predictions] *= np.exp(alpha)\n",
    "        weights[~wrong_predictions] *= np.exp(-alpha)\n",
    "        weights /= sum(weights)\n",
    "        \n",
    "        trees.append(tree)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        \n",
    "        \n",
    "    return trees, alphas\n",
    "\n",
    "def ab_fit_ada(n_trees, X_train, y_train, positive_class=1):\n",
    "    \n",
    "    # Деревья будем записывать в список\n",
    "    trees = []\n",
    "    alphas = []\n",
    "    \n",
    "    weights = np.array([1/X_train.shape[0]] * X_train.shape[0])\n",
    "    \n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        \n",
    "#         tree = build_tree(X_train, y_train, weights, max_depth=1)\n",
    "        tree=DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "        tree.fit(X_train, y_train, sample_weight=weights)\n",
    "        predictions = tree.predict(X_train)\n",
    "#         predictions = np.array(predict(X_train, tree))\n",
    "        wrong_predictions = predictions != y_train\n",
    "        error = sum(weights[wrong_predictions])\n",
    "        alpha = amount_of_say(error)\n",
    "        weights[wrong_predictions] *= np.exp(alpha)\n",
    "        weights[~wrong_predictions] *= np.exp(-alpha)\n",
    "        weights /= sum(weights)\n",
    "        \n",
    "        trees.append(tree)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        \n",
    "        \n",
    "    return trees, alphas\n",
    "\n",
    "def ab_predict_proba(X, trees, alphas, positive_class=1):\n",
    "    predictions = []\n",
    "    for tree in trees:\n",
    "        predictions.append(predict(X, tree))\n",
    "\n",
    "    # сформируем список с предсказаниями для каждого объекта\n",
    "    predictions_per_object = list(zip(*predictions))\n",
    "\n",
    "    # выберем в качестве итогового предсказания вероятности для каждого объекта отношение количества объектов 1 класса ко всем объектам\n",
    "    final_predictions = []\n",
    "    for obj in predictions_per_object:\n",
    "        classes = {}\n",
    "        for i in range(len(obj)):\n",
    "            if obj[i] not in classes:\n",
    "                classes[obj[i]] = 0\n",
    "            classes[obj[i]] += alphas[i]\n",
    "        final_predictions.append(classes.get(positive_class, 0)/sum(classes.values()))\n",
    "\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 16s, sys: 2.01 s, total: 4min 18s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ab_trees, alphas = ab_fit(n_trees=100, X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Leaf at 0x10dc8b820>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_trees[0].false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8915"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = ab_predict(X_train, ab_trees, alphas)\n",
    "accuracy_score(y_train, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8795"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = ab_predict(X_test, ab_trees, alphas)\n",
    "accuracy_score(y_test, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7444640854098229"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pr = ab_predict_proba(X_train, ab_trees, alphas)\n",
    "roc_auc_score(y_train, train_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7490971152507908"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pr = ab_predict_proba(X_test, ab_trees, alphas)\n",
    "roc_auc_score(y_test, test_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.head()\n",
    "XX_test = df.drop(columns=['Id', 'history', 'english']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_answers_proba = ab_predict_proba(XX_test, ab_trees, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission['choose'] = submit_answers_proba\n",
    "sample_submission.to_csv('subm3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614733911674778\n",
      "0.8424003170416989\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(random_state=42, n_estimators=50)\n",
    "ada.fit(X_train, y_train)\n",
    "ada_y_tr = ada.predict_proba(X_train)\n",
    "ada_y_test = ada.predict_proba(X_test)\n",
    "print(roc_auc_score(y_train, ada_y_tr[:, 1]))\n",
    "print(roc_auc_score(y_test, ada_y_test[:, 1]))\n",
    "# print(accuracy_score(y_train, ada.predict(X_train)))\n",
    "# print(accuracy_score(y_test, ada.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8750898139365384\n",
      "0.8519056706587815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "gb_y_tr = gb.predict_proba(X_train)\n",
    "gb_y_test = gb.predict_proba(X_test)\n",
    "print(roc_auc_score(y_train, gb_y_tr[:, 1]))\n",
    "print(roc_auc_score(y_test, gb_y_test[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:14:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9424738505754554\n",
      "0.833456391433269\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(random_state=42, objective='binary:logistic', learning_rate=0.005, n_estimators=3500)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_y_tr = xgb.predict_proba(X_train)\n",
    "xgb_y_test = xgb.predict_proba(X_test)\n",
    "print(roc_auc_score(y_train, xgb_y_tr[:, 1]))\n",
    "print(roc_auc_score(y_test, xgb_y_test[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50527065, 0.49472935],\n",
       "       [0.5009871 , 0.4990129 ],\n",
       "       [0.49878879, 0.50121121],\n",
       "       ...,\n",
       "       [0.50196228, 0.49803772],\n",
       "       [0.50446982, 0.49553018],\n",
       "       [0.51477505, 0.48522495]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = sum(estimator.predict_proba(X_train) * w for estimator, w in zip(ada.estimators_, ada.estimator_weights_))\n",
    "proba /= ada.estimator_weights_.sum()\n",
    "proba = np.exp((1. / (ada.estimators_[0].tree_.n_classes - 1)) * proba)\n",
    "normalizer = proba.sum(axis=1)[:, np.newaxis]\n",
    "# normalizer[normalizer == 0.0] = 1.0\n",
    "proba /= normalizer\n",
    "proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2974586 ],\n",
       "       [3.29769135],\n",
       "       [3.29777257],\n",
       "       ...,\n",
       "       [3.29744401],\n",
       "       [3.29784897],\n",
       "       [3.2975267 ]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8440468580082516"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50783137, 0.49216863],\n",
       "       [0.50316761, 0.49683239],\n",
       "       [0.50057549, 0.49942451],\n",
       "       ...,\n",
       "       [0.50230715, 0.49769285],\n",
       "       [0.50695841, 0.49304159],\n",
       "       [0.51913141, 0.48086859]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_y_tr = ada.predict_proba(X_train)\n",
    "ada_y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_predict_proba(X, trees, alphas, positive_class=1):\n",
    "    predictions = []\n",
    "    for tree in trees:\n",
    "        predictions.append(predict_proba(X, tree))\n",
    "        \n",
    "    alphas = np.array(alphas)\n",
    "    weights = np.ones(alphas.shape[0])\n",
    "\n",
    "    # сформируем список с предсказаниями для каждого объекта\n",
    "    predictions_per_object = list(zip(*predictions))\n",
    "#     print(predictions_per_object)\n",
    "\n",
    "    # выберем в качестве итогового предсказания вероятности для каждого объекта отношение количества объектов 1 класса ко всем объектам\n",
    "    final_predictions = []\n",
    "    for obj in predictions_per_object:\n",
    "#         classes = {}\n",
    "#         for i in range(len(obj)):\n",
    "#             if obj[i] not in classes:\n",
    "#                 classes[obj[i]] = 0\n",
    "#             classes[obj[i]] += alphas[i]\n",
    "#         final_predictions.append(classes.get(positive_class, 0)/sum(classes.values()))\n",
    "        obj = np.array(obj)\n",
    "        w_pr_pos = np.array(sum(obj * weights) / sum(weights))\n",
    "        w_pr_not_pos = np.array(sum((1 - obj) * weights) / sum(weights))\n",
    "        final_predictions.append(np.exp(w_pr_pos) / (np.exp(w_pr_pos) + np.exp(w_pr_not_pos)))\n",
    "        \n",
    "    final_predictions = np.array(final_predictions)\n",
    "\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.array(alphas)\n",
    "weights = np.ones(alphas.shape[0])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "y = df['choose']\n",
    "X = df.drop(columns=['choose', 'Id', 'history', 'english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 26 ms, total: 2.69 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "weights = np.array([1/X_train.shape[0]] * X_train.shape[0])\n",
    "tree = build_tree(X_train, y_train, weights, max_depth=1)\n",
    "#         tree=DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "#         tree.fit(X_train, y_train, sample_weight=weights)\n",
    "#         predictions = tree.predict(X_train)\n",
    "predictions = np.array(predict(X_train, tree))\n",
    "wrong_predictions = predictions != y_train\n",
    "error = sum(weights[wrong_predictions])\n",
    "alpha = amount_of_say(error)\n",
    "weights[wrong_predictions] *= np.exp(alpha)\n",
    "weights[~wrong_predictions] *= np.exp(-alpha)\n",
    "weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X_train, y_train, weights, max_depth=1)\n",
    "#         tree=DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "#         tree.fit(X_train, y_train, sample_weight=weights)\n",
    "#         predictions = tree.predict(X_train)\n",
    "predictions = np.array(predict(X_train, tree))\n",
    "wrong_predictions = predictions != y_train\n",
    "error = sum(weights[wrong_predictions])\n",
    "alpha = amount_of_say(error)\n",
    "weights[wrong_predictions] *= np.exp(alpha)\n",
    "weights[~wrong_predictions] *= np.exp(-alpha)\n",
    "weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=1, random_state=42)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "sk_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "sk_tree.fit(X_train, y_train, sample_weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_predictions = sk_tree.predict(X_train) != y_train\n",
    "error = sum(weights[w_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3505392713053071"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30833481325693723"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_of_say(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4280.17967719, 3719.82032281])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prb = sk_tree.tree_.predict(X_train.astype(np.float32))\n",
    "normalizer = prb.sum(axis=1)[:, np.newaxis]\n",
    "normalizer[normalizer == 0.0] = 1.0\n",
    "prb /= normalizer\n",
    "sum(prb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.066, 0.034],\n",
       "       [0.042, 0.08 ]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ew = np.array([[2, 3]])\n",
    "pr = np.array([[0.33, 0.17], [0.21, 0.40]])\n",
    "pr /= ew.sum()\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(X_train, y_train, weights, max_depth=1)\n",
    "#         tree=DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "#         tree.fit(X_train, y_train, sample_weight=weights)\n",
    "#         predictions = tree.predict(X_train)\n",
    "predictions = np.array(predict(X_train, tree))\n",
    "# wrong_predictions = predictions != y_train\n",
    "# error = sum(weights[wrong_predictions])\n",
    "# alpha = amount_of_say(error)\n",
    "# weights[wrong_predictions] *= np.exp(alpha)\n",
    "# weights[~wrong_predictions] *= np.exp(-alpha)\n",
    "# weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_gini(y_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4978763171929792"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_gini(tree.false_branch.labels, tree.false_branch.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.true_branch.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.5       ]],\n",
       "\n",
       "       [[0.33321649, 0.18375576]],\n",
       "\n",
       "       [[0.16678351, 0.31624424]]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_tree.tree_.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
